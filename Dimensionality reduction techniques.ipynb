{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average ratio of Euclidean distances between the reduced and original dataset is 0.9760716811855876\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Generating 20-dimensional dataset with size N = 100. This will be our original dataset.\n",
    "# Each entry of the dataset will be selected at random from the N(0,1) distribution.\n",
    "# As our dataset is of size (100) and each vector is of dimension (20), we will have a total of 2000 datapoints.\n",
    "# This dataset is labelled 'M'.\n",
    "\n",
    "def dataset(d, e):\n",
    "    m = []\n",
    "    mLength = d\n",
    "    for i in range(mLength):\n",
    "        m.append(np.random.normal(0, 1, e))\n",
    "    return m\n",
    "\n",
    "M = dataset(100, 20)\n",
    "\n",
    "# Defining parameter for JLT: using 3 values for epsilon yielding the lower bound for reduced dimension values. \n",
    "# We will take the smallest integer above the lower bound to calculate the reduced dimensions.\n",
    "# In theory, this reduced dimension should be of size inferior to the original high dimension dataset. In this case,\n",
    "# it is actually the opposite that we are seeing. This is probably due to the fact that our original dataset is only \n",
    "# of dimension 20. This could mean that, for the JLT Transform to really be effective, we should only apply it on \n",
    "# very high dimension database\n",
    "\n",
    "epsilon = [0.4, 0.3, 0.1]\n",
    "dim_reduced = []\n",
    "for i in epsilon:\n",
    "    dim = int(np.ceil((24 * np.log(20)) / (3 * pow(i,2) - 2 * pow(i,3))))\n",
    "    dim_reduced.append(dim)\n",
    "e_index = 0\n",
    "\n",
    "# Using epsilon = (0.4, 0.3, 0.1), we find a lower bound for the reduced dimension equal to (205, 333, 2568),\n",
    "# respectively. Now let's generate a Gaussian Matrix with size = dim_reduced[i], 20. Each entry is selected \n",
    "# at random from the N(0,1) distribution. This Gaussian Matrix is labelled 'G'\n",
    "\n",
    "G = dataset(dim_reduced[e_index], 20)\n",
    "\n",
    "# applying the JLT transform. To do this, we simply multiply each vector of dimension N=20 in M by each row of G. \n",
    "# We also multiply this result by 1 divided by the square root of dim_reduced. Applying the JLT yields a dataset\n",
    "# constructed of 100 vectors with dimension dim_reduced = (205, 333 or 2568) depending on the epsilon we select.\n",
    "# We start by multiplying each vector of G by (1/sqrt(dim_reduced)):\n",
    "\n",
    "F = []\n",
    "for i in range(dim_reduced[e_index]):\n",
    "    F_i = []\n",
    "    for x in G[i]:\n",
    "        a = x * (1/np.sqrt(dim_reduced[e_index]))\n",
    "        F_i.append(a)\n",
    "    F.append(F_i)\n",
    "\n",
    "# We now apply the JLT by computing the dot product of M - our original dataset - and F - our dimensionality \n",
    "# reduction matrix -, which will yield a new dataset with dimension dim_reduced.\n",
    "    \n",
    "JLT = []\n",
    "for i in range(len(M)):\n",
    "    Y = []\n",
    "    for j in range(len(F)):\n",
    "        y = 0\n",
    "        for k in range(20):\n",
    "            y += float(F[j][k]) * float(M[i][k])\n",
    "        Y.append(y)    \n",
    "    JLT.append(Y)\n",
    "    \n",
    "# Note: we used list of lists until now. We will transform those in matrices to use the euclidean distance feature\n",
    "# of the numpy library (this calculation requires the usage of matrices)\n",
    "\n",
    "JLT_matrix = np.matrix(JLT)\n",
    "M_matrix = np.matrix(M)\n",
    "\n",
    "# Now that we have this (supposedly) reduced dimension dataset, we need to make sure that the error is, with high \n",
    "# probability, not greater than a certain threshold (that depends on our choice of epsilon). This error can be measured\n",
    "# by computing the Euclidean distance between 2 vectors of our reduced dataset and comparing it to that of the original\n",
    "# dataset. We will do just that below\n",
    "\n",
    "def dist(mat1, mat2, ite):\n",
    "    result_noJLT = []\n",
    "    result_JLT = []\n",
    "    result = []\n",
    "    for i in range(50):\n",
    "        a = random.randint(0,ite-1)\n",
    "        b = random.randint(0,ite-1)\n",
    "        while a == b:\n",
    "            b = random.randint(0,ite-1)\n",
    "        dist_noJLT = np.linalg.norm(mat1[a,:] - mat1[b,:])       # calculation of the euclidean distance\n",
    "        dist_JLT = np.linalg.norm(mat2[a,:] - mat2[b,:])\n",
    "        result_noJLT.append(dist_noJLT)\n",
    "        result_JLT.append(dist_JLT)\n",
    "        result_ratio = dist_JLT / dist_noJLT\n",
    "        result.append(result_ratio)\n",
    "        res = 'The average ratio of Euclidean distances between the reduced and original dataset is {}'.format(np.average(result))\n",
    "    return res \n",
    "\n",
    "euclid_distance = dist(M_matrix, JLT_matrix, 100)\n",
    "print(euclid_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different matrix that we have are: M, G, JLT. Those matrix have sizes, respectively: (100 x 20), (j x 20), and  (j x 100), where j is equal to the reduced dimension and depends on the choise of epsilon.\n",
    "\n",
    "The lower bound of reduced dimensions using epsilon = (0.4 , 0.3, 0.1) are equal to (205, 333, 2568).\n",
    "\n",
    "We see that using an epsilon equal to 0.4, 0.3, and 0.1, the JLT technique will yield an average ratio of the euclidian distances (reduced / original) of ~ 1 in all three cases. This means that the distance is well conserved and proves the efficiency of the JLT technique.\n",
    "\n",
    "What is interesting to note is that the JLT technique actually increases the dimension of the dataset instead of reducing it. If we increase the dimensionality of our original dataset - M -, to a high dimension (like 1000) we can see that the JLT does reduce the dimensionality of the resulting JLT. This reduction is inversely proportional to epsilon, i.e. if epsilon is close to one, the reduced dimension is close to that of the original dataset. The opposite holds also, if epsilon is close to 0, the resulting dimension post-JLT is considerably reduced.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposing a variant of the JLT transform, where entries of the dimensionality reduction matrix are taken uniformly at random from the normal distribution N(0,1). In many practical applications this variant is often simplified by replacing Gaussian entries by entries taken from the discrete set {−1, +1} (each value with probability 1/2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average ratio of Euclidean distances between the reduced and original dataset is 1.0057460666282472\n"
     ]
    }
   ],
   "source": [
    "# Creating the dimensionality reduction matrix with entries equal to {-1,1}, both values with probability 0.5.\n",
    "# This matrix is labelled 'G_ex2' and has dimension dim_reduced[e_index].\n",
    "\n",
    "def dataset2(d, e):\n",
    "    m = []\n",
    "    mLength = d\n",
    "    for i in range(mLength):\n",
    "        m.append(np.random.choice([-1, 1], e))\n",
    "    return m\n",
    "\n",
    "G_ex2 = dataset2(dim_reduced[e_index], 20)    # this will be our dimensionality reduction matrix\n",
    "M_ex2 = dataset(100, 20)\n",
    "\n",
    "# Applying the JLT to our original dataset 'M', using 'G_ex2' and our dim_reduced (reduced dimension)\n",
    "\n",
    "F_ex2 = []\n",
    "for i in range(dim_reduced[e_index]):\n",
    "    F_i = []\n",
    "    for x in G_ex2[i]:\n",
    "        a = x * (1/np.sqrt(dim_reduced[e_index]))\n",
    "        F_i.append(a)\n",
    "    F_ex2.append(F_i)\n",
    "\n",
    "# We now apply the JLT by computing the dot product of M - our original dataset - and F - our dimensionality \n",
    "# reduction matrix -, which will yield a new dataset with dimension dim_reduced.\n",
    "    \n",
    "JLT_ex2 = []\n",
    "for i in range(len(M_ex2)):\n",
    "    Y = []\n",
    "    for j in range(len(F_ex2)):\n",
    "        y = 0\n",
    "        for k in range(20):\n",
    "            y += float(F_ex2[j][k]) * float(M_ex2[i][k])\n",
    "        Y.append(y)    \n",
    "    JLT_ex2.append(Y)\n",
    "    \n",
    "# Note: we used list of lists until now. We will transform those in matrices to use the euclidean distance feature\n",
    "# of the numpy library (this calculation requires the usage of matrices)\n",
    "\n",
    "JLT2_matrix = np.matrix(JLT_ex2)\n",
    "M2_matrix = np.matrix(M_ex2)\n",
    "\n",
    "euclid_distance = dist(M2_matrix, JLT2_matrix, 100)\n",
    "print(euclid_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of the structured JLT, instead of using a gaussian matrix to apply the JLT, we will use a matrix that is made of entries randomely selected from the set {1; -1}, with probability 0.5 for each value. Thus to reduce dimensionality, we will come up with the proper D (where D = dimension after reduction, and where d depends on our choice of epsilon, the precision parameter). We will then create a reduction matrix of size (D x N) (where N is the dimension of the original database) and fill it with entries randomly selected in the {1; -1} set (again with probability 0.5 for each value). We will then effectuate the dot product between the reduction matrix and each vector (of dimension N) of the origial higher dimesnional dataset, and we will then multiply those resulting vectors by the scalar (1 / square root(D)). This will provide us with a dataset of reduced dimension (dimension D) that will conserve on expectation the same Euclidean distance between two vectors x,y, taken from the reduced dimension as that from the higher original dimenson. Since the dimensionality reduction matrix is made of entries from the set {-1,1} selected with equal probability, and that we are calculating the square of distances, the JLT should in fact have no effect on the euclidean distance of both datasets. We should thus, on expectation, have the same Euclidean distance for both datasets. \n",
    "\n",
    "\n",
    "We see that using an epsilon equal to 0.4, 0.3, and 0.1, the structured JLT technique will yield an average ratio of the euclidian distances (reduced / original) of ~ 1 in all three cases. This means that the distance is well conserved and proves the efficiency of the JLT technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Toeplitz matrix A ∈ Rn×m is a matrix, which entries do not change along each diagonal. In other words, the Toeplitz matrix is defined by determining its first row and column. In the JLT setting Toeplitz matrices can be used to improve the quality of the dimensionality reduction embedding since they provide the increase of the “budget of randomness” used. Testing the structured JLT, where Gaussian circulant projection matrix P is replaced by a Toeplitz Gaussian projection matrix (i.e. you independently sample n + m − 1 values from the normal distribution N (0, 1) and using them to define the first row and column of the Toeplitz projection matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average ratio of Euclidean distances between the reduced and original dataset is 1.0737008229602576\n"
     ]
    }
   ],
   "source": [
    "# Let's create the Toeplitz Gaussian projection matrix:\n",
    "\n",
    "def Toeplitz(m,n):\n",
    "    a = []\n",
    "    a.append([])\n",
    "    for i in range (n):\n",
    "        a[0].append(np.random.normal(0,1))\n",
    "    a.append([])\n",
    "    for i in range (m-1):\n",
    "        a.append([])\n",
    "        a[i+1].append(np.random.normal(0,1))\n",
    "    s = 0\n",
    "    for row in range(m-1):\n",
    "        s += 1\n",
    "        for col in range(n-1):\n",
    "            a[s].append(a[s-1][col])\n",
    "    a.pop()    # for an unknown reason, my method appends an empty list at the end of my matrix. I remove it here\n",
    "    return a\n",
    "\n",
    "    \n",
    "G_ex3 = Toeplitz(dim_reduced[e_index], 20)\n",
    "M_ex3 = dataset(100, 20)\n",
    "\n",
    "# Applying the JLT to our original dataset 'M', using 'G_ex3' and our dim_reduced (reduced dimension)\n",
    "\n",
    "F_ex3 = []\n",
    "for i in range(dim_reduced[e_index]):\n",
    "    F_i = []\n",
    "    for x in G_ex3[i]:\n",
    "        a = x * (1/np.sqrt(dim_reduced[e_index]))\n",
    "        F_i.append(a)\n",
    "    F_ex3.append(F_i)\n",
    "\n",
    "# We now apply the JLT by computing the dot product of M - our original dataset - and F - our dimensionality \n",
    "# reduction matrix -, which will yield a new dataset with dimension dim_reduced.\n",
    "    \n",
    "JLT_ex3 = []\n",
    "for i in range(len(M_ex3)):\n",
    "    Y = []\n",
    "    for j in range(len(F_ex3)):\n",
    "        y = 0\n",
    "        for k in range(20):\n",
    "            y += float(F_ex3[j][k]) * float(M_ex3[i][k])\n",
    "        Y.append(y)    \n",
    "    JLT_ex3.append(Y)\n",
    "    \n",
    "# Note: we used list of lists until now. We will transform those in matrices to use the euclidean distance feature\n",
    "# of the numpy library (this calculation requires the usage of matrices)\n",
    "\n",
    "JLT3_matrix = np.matrix(JLT_ex3)\n",
    "M3_matrix = np.matrix(M_ex3)\n",
    "\n",
    "euclid_distance = dist(M3_matrix, JLT3_matrix, 100)\n",
    "print(euclid_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the normal steps of the JLT, but this time using the Toeplitz Gaussian matrix to perform the reduction step. We simulate a Gaussian matrix using only (n+m-1) random variables. Those variables represent the entries in column and row 0 of the matrix. We then use those (n+m-1) random variables (RV ~ N(0,1)) to produce each diagonale in the matrix. This allows us to reduce the amount of random variables simulated and stored in memory, which comes in handy when handling much larger datasets. We then use this Toeplitz matrix to compute the dot product with each vector of the original dataset and then multiply the result by a scalar equal to (1 / square root(d)) (where d is equal to the reduced dimension). This provides us with a new dataset with reduced dimension d.\n",
    "\n",
    "The results, when computing the ratio between the Euclidean distance of the reduced dataset vs. the original dataset is ~ 0.99, for all epsilons, where epsilon is = (0.4, 0.3, 0.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

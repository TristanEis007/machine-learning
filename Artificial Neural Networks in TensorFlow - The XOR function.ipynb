{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Networks Applications Using TensorFlow \n",
    "<br> Prepared for the Aritfical Neural Network Seminar at Columbia University, Sep 2017\n",
    ">Author: Tristan Eisenhart\n",
    "<br>te2252@columbia.edu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you haven't installed TensorFlow on your machine yet, please follow those intructions: https://www.tensorflow.org/install/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Introduction to Neural Networks and TensorFlow -- currently using version 1.2 of TensorFlow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graphs, variables and operations\n",
    "We'll learn through a couple of examples, but before jumping into the practice, let's have a look at a couple of features that should be understood before starting to develop in TensorFlow:\n",
    "\n",
    "1. Graphs -- a computational graph in TensorFlow is a series of operations arranged into a graph of nodes. The development of Neural Networks algorithms in TensorFlow is done through two distinct steps: \n",
    "> 1. building the computational graph, which focuses on creating and defining the nodes of the graph\n",
    "> 1. running the computational graph, which focuses on evaluating the resulting graph through what is called a session\n",
    "\n",
    "1. Variables -- multiple type of variables exist in TensorFlow -- the most common ones are presneted below:\n",
    "\n",
    "    > Constant variables: float or int variables that will remain constant and that we wish to declare in step1 <br>\n",
    "    > Zeros: tensors with a specific shape that are initiated with zeros <br>\n",
    "    > Placeholders: tensors for which we will pass a value in the future. We need to specify a shape when declaring placeholders <br>\n",
    "\n",
    "1. Operations -- TensorFlow has built-in functions for basic and complex operations\n",
    "> Addition: tf.add() <br>\n",
    "> Matrix multiplication: tf.matmul() <br>\n",
    "> ... <br>\n",
    "> But also prebuilt functions, optimization methods, network designs, cells (LSTM), etc. <br>\n",
    "> The TensorFlow documentation can be found at https://www.tensorflow.org/api_docs/python/\n",
    "    \n",
    "Now that we have (very) quickly defined those 3 elements of TensorFlow, let's look at an example that illustrate the concept of the session. Once again, a session is used to evaluate the graph that is built in the first step mentioned above.\n",
    "<br>\n",
    "##### Step 1: building the computational graph\n",
    "Here, we are simply passing variables to the graph and multiplying the matrices node3 and node4. Notice that when you run the cell below, TensorFlow does not display the value of the variables but their characteristics. To get an \n",
    "overview of their values, you will have to evaluate the graph that you have created in step 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_4:0\", shape=(), dtype=float32)\n",
      "Tensor(\"zeros_4:0\", shape=(2, 3), dtype=int32)\n",
      "Tensor(\"Placeholder_4:0\", shape=(2, 3), dtype=float32)\n",
      "Tensor(\"random_normal_4:0\", shape=(3, 2), dtype=float32)\n",
      "Tensor(\"MatMul_4:0\", shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# equivalent to node1 = 7.0\n",
    "node1 = tf.constant(7.0, dtype = tf.float32) \n",
    "\n",
    "# tensor of shape (2,3) initiated with zeros\n",
    "node2 = tf.zeros((2,3), dtype = tf.int32) \n",
    "\n",
    "# a placeholder is an empty tensor with a specified shape and type for which we will pass values in the future (when evaluating the graph)\n",
    "node3 = tf.placeholder(dtype = tf.float32, shape = (2,3)) \n",
    "\n",
    "# a matrix with size [3,2] and random gaussian variables\n",
    "node4 = tf.random_normal([3, 2], seed=1234)\n",
    "\n",
    "# the matrix multiplication operator\n",
    "node5 = tf.matmul(node3,node4)\n",
    "\n",
    "# Displaying the type of our variables\n",
    "print(node1)\n",
    "print(node2)\n",
    "print(node3)\n",
    "print(node4)\n",
    "print(node5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: evaluating the computational graph through a session\n",
    "Now that when we want to evaluate the variables passed into our graph, we will call a session using tf.Session(). Notice that in step 1, we had created a placeholder variable. A placeholder is an empty tensor with a prespecified shape and type for which we will pass a value in the future, when evaluating the graph. When you will develop more advanced neural networks in which you will want to pass training data, you will need to use placeholders to train your graph in batches (more on this later). For now, notice how when you run the below cell, the true values of the variables are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1:\n",
      " 7.0\n",
      "node2:\n",
      " [[0 0 0]\n",
      " [0 0 0]]\n",
      "node3:\n",
      " [[ 0.  1.  2.]\n",
      " [ 3.  4.  5.]]\n",
      "node4:\n",
      " [[ 0.51340485 -0.25581399]\n",
      " [ 0.65199131  1.39236379]\n",
      " [ 0.37256798  0.20336303]]\n",
      "node5:\n",
      " [[  3.55750608   2.70003891]\n",
      " [ 12.15140343   9.36564636]]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Running the computational graph\n",
    "\n",
    "# Calling a session to evaluate the graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Creating some data to feed into our placeholder \n",
    "x = [[0,1,2],[3,4,5]]\n",
    "\n",
    "# Displaying the values of our variables, by running the graph using sess.run(feed_dict={})\n",
    "print(\"node1:\\n\", sess.run(node1))\n",
    "print(\"node2:\\n\", sess.run(node2))\n",
    "\n",
    "# This is how you will want to feed values into your placeholders when running your graph\n",
    "print(\"node3:\\n\", sess.run(node3, feed_dict={node3:x}))\n",
    "print(\"node4:\\n\", sess.run(node4))\n",
    "print(\"node5:\\n\", sess.run(node5, feed_dict={node3:x}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our very short introduction to using TensorFlow. A lot of prebuilt funtions and operations are explained and described in the TensorFlow documentation that can be found at https://www.tensorflow.org/api_docs/python/. Let's now look at our first application, the exclusive-or (XOR) function. This is a nonlinear problem that is quite difficult to solve. For a reminder of what the XOR function is, you can visit https://en.wikipedia.org/wiki/Exclusive_or. As you will notice in the 2D graph below, linear functions do not work to approximate the XOR function, as there is no way to seperate the data linearly (try to draw a line that seperates the black dotes from the grey dotes and you will see that this is not possible). \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Michael_Siomau/publication/232642531/figure/fig1/AS:300360385220613@1448622904913/FIG-1-The-feature-space-of-XOR-function-is-two-dimensional-and-discrete-each-feature.png\">\n",
    "\n",
    "<i> source: https://www.researchgate.net/profile/Michael_Siomau/publication/232642531/figure/fig1/AS:300360385220613@1448622904913/FIG-1-The-feature-space-of-XOR-function-is-two-dimensional-and-discrete-each-feature.png </i>\n",
    "\n",
    "That is why Neural Networks (that rely on a nonlinear activation function) are very good at approximating the XOR function. Let's take a look at how a simple Feed-Forward Neural Net can be developed and applied in the context of the XOR function using TensorFlow.\n",
    "\n",
    "### Application 1 - The XOR function\n",
    "\n",
    "Step 0: Let's create some data for our first application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's take a look at our data:\n",
      "    output  x1  x2  x3\n",
      "0       1   0   0   1\n",
      "1       1   1   0   1\n",
      "2       0   0   0   0\n",
      "3       1   0   1   0\n",
      "4       0   1   1   1\n",
      "5       0   1   1   1\n",
      "6       1   1   0   1\n"
     ]
    }
   ],
   "source": [
    "# Importing relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Our dataset consist of 7 samples with 3 binary features each and a binary output value (either 0 or 1)\n",
    "df = pd.DataFrame({'x1':np.array([0,1,0,0,1,1,1]),'x2':np.array([0,0,0,1,1,1,0]),\n",
    "                   'x3':np.array([1,1,0,0,1,1,1]),'output':np.array([1,1,0,1,0,0,1])})\n",
    "\n",
    "print(\"Let's take a look at our data:\\n\",df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical neural network is made of layers, nodes, weights and biases (see the image below). In the feed-forward architecture, information flows forward without going backwards. The below image displays an input layer, two hidden layers and an output layer.\n",
    "<img src=\"https://camo.githubusercontent.com/269f47b8185a2ca349ead57db511250553fd918b/687474703a2f2f63733233316e2e6769746875622e696f2f6173736574732f6e6e312f6e657572616c5f6e6574322e6a706567\"> </img>\n",
    "\n",
    "<i> source: <a html=\"https://camo.githubusercontent.com/269f47b8185a2ca349ead57db511250553fd918b/687474703a2f2f63733233316e2e6769746875622e696f2f6173736574732f6e6e312f6e657572616c5f6e6574322e6a706567 \"> link </a> </i>\n",
    "\n",
    "At each layer, you will encounter an activation function (adding nonlinearity), and a matrix multiplication of the sort:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$ layer\\ output = \\sigma(X.W + Biases),\\ where\\ \\sigma\\ is\\ a\\ nonlinear\\ activation\\ function$$\n",
    "\n",
    "<br>\n",
    "For a list of all activation functions supported in TensorFlow, visit https://www.tensorflow.org/versions/r0.12/api_docs/python/nn/activation_functions_ <br>\n",
    "\n",
    "When optimizing a neural network's parameters, it is necessary to come up with a cost function. This cost function, normally representing the error associated with the classification or prediction task (such as wrongly classified observations, or by how far our prediction misses the observed output) will be minimized using optimization algorithms. Typically, the Adam algorithm or the Stochastic Gradient Descent Algorithms can be used to come up with optimal (not necessarily global optimal) values for our weights and biases parameters.\n",
    "\n",
    "In our example, we will use the Mean Square Error, equal to: <br>\n",
    "\n",
    "$$ MSE = \\Sigma(y - \\widehat{y})^2$$ <br>\n",
    "\n",
    "In other words, the closer our prediction is from the right class, the smaller our mean squared error will be. To optimize our parameters, we will use the stochastic gradient descent method. We'll run 10,000 iterations to minimize our Mean Squared Error. The architecture of the network will be a single layer with 4 nodes and we will predict a continous variable. At the end of our while loop, we hope to have output values close to 1 for observations that have a class = 1 in our input data and values close to 0 for observations that have a class = 0. We will use a sigmoid acitvation function and we will initialize our weights and biases using Gaussian random variables. The learning rate that we will use is equal to 0.01 (think of the learning rate as how far our gradient descent will take a leap at each iteration). Notice that we will have a matrix of size [number of input feautres, number of nodes] to represent our weights in the hidden layer and a matrix of size [number of nodes, number of classes] for our output layer. \n",
    "\n",
    "With all that said, let's dig into how we build our graph.\n",
    "\n",
    "##### Step 1: building the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Resetting to default graph -- especially usefull when running multiple sessions\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Declaring parameters / architecture of neural network\n",
    "num_input_features = 3 # represents the number of features in the input data\n",
    "num_hidden_nodes = 4 # the number of nodes used in the 1st (and only) hidden layer of our network\n",
    "num_classes = 1 # the number of features in the output data -- this is equivalent to a regression problem, we are not trying to predict a class but a number, therefore there is only 1 class\n",
    "learning_rate = 0.01 # parameter used in the optimization process\n",
    "seed = 7 # to replicate results\n",
    "\n",
    "# Declaring placeholders for input data and true outputs\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, 3]) # inputs size will be size of dataset * num_input_features\n",
    "true_outputs = tf.placeholder(tf.float32, shape=[None, 1]) # output size will be size of dataset * num_classes\n",
    "\n",
    "# Randomely initializing weights and biases using normal distribution\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([num_input_features, num_hidden_nodes], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([num_hidden_nodes, num_classes], seed=seed))}\n",
    "\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([num_hidden_nodes], seed=seed)),\n",
    "    'output': tf.Variable(tf.random_normal([num_classes], seed=seed))}\n",
    "\n",
    "# Computing layer_1 and the output layer (this is a single-layer feed forward neural net) with a sigmoid activation function\n",
    "# The introduction of an activation function allows for non-linearity\n",
    "# Layers are simply equal to activation_function(Wx + biases)\n",
    "\n",
    "layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(inputs,weights['hidden']),biases['hidden']))\n",
    "output_layer = tf.nn.sigmoid(tf.add(tf.matmul(layer_1,weights['output']),biases['output']))\n",
    "\n",
    "# Now that the architecture is designed, let's look at the optimization process -- our objective / cost / error function is the mean square error \n",
    "# We use an iterative optimization process, here the Stochastic Gradient Descent Methode that learns at a predefined learning_rate\n",
    "error = tf.subtract(output_layer, true_outputs)\n",
    "mean_square_error = tf.reduce_sum(tf.square(error))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(mean_square_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all we have done is create some weights and biases variables, declared input placeholders and expressed the equations that will be used to formulate the network as well as to optimize the parameters. Now let's jump to step 2, in which we will run our 10,000 iterations and display our results.\n",
    "\n",
    "##### Step 2: Running our computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimization\n",
      "Iteration: 0 Mean_square_error: 2.26706 \n",
      "Output\n",
      " [[ 0.83837086]\n",
      " [ 0.88519567]\n",
      " [ 0.80585104]\n",
      " [ 0.77517498]\n",
      " [ 0.87023813]\n",
      " [ 0.87023813]\n",
      " [ 0.88519567]]\n",
      "Iteration: 2000 Mean_square_error: 1.40898 \n",
      "Output\n",
      " [[ 0.66873139]\n",
      " [ 0.70373935]\n",
      " [ 0.59839165]\n",
      " [ 0.3687821 ]\n",
      " [ 0.42848191]\n",
      " [ 0.42848191]\n",
      " [ 0.70373935]]\n",
      "Iteration: 4000 Mean_square_error: 1.06147 \n",
      "Output\n",
      " [[ 0.76707333]\n",
      " [ 0.8082152 ]\n",
      " [ 0.41577893]\n",
      " [ 0.26200432]\n",
      " [ 0.32874194]\n",
      " [ 0.32874194]\n",
      " [ 0.8082152 ]]\n",
      "Iteration: 6000 Mean_square_error: 0.797579 \n",
      "Output\n",
      " [[ 0.85751957]\n",
      " [ 0.88369256]\n",
      " [ 0.28574413]\n",
      " [ 0.2936894 ]\n",
      " [ 0.29128924]\n",
      " [ 0.29128924]\n",
      " [ 0.88369256]]\n",
      "Iteration: 8000 Mean_square_error: 0.425461 \n",
      "Output\n",
      " [[ 0.89129716]\n",
      " [ 0.89529473]\n",
      " [ 0.25673971]\n",
      " [ 0.53540641]\n",
      " [ 0.23447354]\n",
      " [ 0.23447354]\n",
      " [ 0.89529473]]\n",
      "Iteration: 10000 Mean_square_error: 0.179643 \n",
      "Output\n",
      " [[ 0.90059078]\n",
      " [ 0.91688555]\n",
      " [ 0.17294322]\n",
      " [ 0.72849256]\n",
      " [ 0.16173878]\n",
      " [ 0.16173878]\n",
      " [ 0.91688555]]\n",
      "Very cool, we are finished with the optimiztion!\n"
     ]
    }
   ],
   "source": [
    "# Creating a session to run the graph\n",
    "sess = tf.Session()\n",
    "\n",
    "# Initializing all variables - you need to initialize your variables before running your graph\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# Let's limit the number of iterations \n",
    "iter_ = 0\n",
    "print(\"Starting optimization\")\n",
    "\n",
    "while iter_ <= 10000:\n",
    "    \n",
    "    # Here we are running the optimization using the Stochastic Gradient Descent Methode. Remember that we \n",
    "    # need to feed input data to our placeholders\n",
    "    _ = sess.run(train, feed_dict={inputs:np.array(df[['x1','x2','x3']]),true_outputs:np.array(df[['output']])})\n",
    "    \n",
    "    # Evaluating the Mean Squared Error\n",
    "    mse = sess.run(mean_square_error, feed_dict={inputs:np.array(df[['x1','x2','x3']]),true_outputs:np.array(df[['output']])})\n",
    "    \n",
    "    # Displaying results every 2000 iterations\n",
    "    if iter_ % 2000 == 0:\n",
    "        # Evaluating the output layer -- what is predicted for each observation\n",
    "        out = sess.run(output_layer, feed_dict={inputs:np.array(df[['x1','x2','x3']])})\n",
    "        \n",
    "        # Displaying the mean square error\n",
    "        print(\"Iteration:\",iter_, \"Mean_square_error:\",mse, \"\\nOutput\\n\",out)\n",
    "    \n",
    "    iter_ += 1\n",
    "\n",
    "print(\"Very cool, we are finished with the optimiztion!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we start with a Mean Squared Error Function greater than 2.2. As iterations are run, and as the network learns the XOR function, our model makes predictions that are closer and closer to the real output values of our observations. In turn, this leads to a cost function that decreases with iterations. In the final run, notice how our predictions seperate: observations 1,2,4 and 7 are converging to their true value of 1, while observations 3,5 and 6 converge to their true value of 0. Our error function is also the smallest at iteration 10,000.\n",
    "\n",
    "We have succesfully approximated the XOR function using a single-layer feed-forward neural network. \n",
    "\n",
    "In the next application, we will take a look at the very famous digit recognition problem using the MNIST dataset. We'll create a network that recognizes handwritten digits and classifies them from 0 to 9. I'll try to publish an update with that application soon.\n",
    "\n",
    "Hope you enjoyed this hsort tutorial to Artificial Neural Networks in TensorFlow :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THAT'S IT FOR NOW"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
